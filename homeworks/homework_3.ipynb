{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"homework_3.ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyMxT/3xPMMrR+MV7DF1q1+/"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"ZrkJTj_AeTpn"},"source":["# Deep Learning - Homework 03\n","\n","### Roberto Corti"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"6gtQvJH9nckF","executionInfo":{"status":"ok","timestamp":1618147622002,"user_tz":-120,"elapsed":30824,"user":{"displayName":"Roberto Corti","photoUrl":"https://lh4.googleusercontent.com/-KaiJHEP6Eps/AAAAAAAAAAI/AAAAAAAAABU/y4Yv-P85NuU/s64/photo.jpg","userId":"08099351609548175475"}},"outputId":"46757d72-7711-40b9-e0da-a94ecd688885"},"source":["### GOOGLE DRIVE \n","\n","from google.colab import drive\n","\n","folder_mount = '/content/drive' # Your Drive will be mounted on top of this path\n","\n","drive.mount(folder_mount)\n","\n","%cd drive/MyDrive/Università/DSSC/Secondo\\ Anno/Deep-Learning/homeworks/"],"execution_count":1,"outputs":[{"output_type":"stream","text":["Mounted at /content/drive\n","/content/drive/MyDrive/Università/DSSC/Secondo Anno/Deep-Learning/homeworks\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"pT8WjCakfJ8r"},"source":["### 1. Implement L1 norm regularization as a custom loss function"]},{"cell_type":"code","metadata":{"id":"Vm9OATNuBUW3","executionInfo":{"status":"ok","timestamp":1618147628364,"user_tz":-120,"elapsed":4346,"user":{"displayName":"Roberto Corti","photoUrl":"https://lh4.googleusercontent.com/-KaiJHEP6Eps/AAAAAAAAAAI/AAAAAAAAABU/y4Yv-P85NuU/s64/photo.jpg","userId":"08099351609548175475"}}},"source":["import torch\n","import torch.nn as nn\n","import torchvision\n","import numpy as np\n","from torchvision.datasets import MNIST\n","import matplotlib.pyplot as plt\n","from torchvision.transforms import ToTensor, Normalize, Compose"],"execution_count":2,"outputs":[]},{"cell_type":"code","metadata":{"id":"m2AtNjd6kuO9","executionInfo":{"status":"ok","timestamp":1618147630382,"user_tz":-120,"elapsed":383,"user":{"displayName":"Roberto Corti","photoUrl":"https://lh4.googleusercontent.com/-KaiJHEP6Eps/AAAAAAAAAAI/AAAAAAAAABU/y4Yv-P85NuU/s64/photo.jpg","userId":"08099351609548175475"}}},"source":["# Fully connected neural network with 3 hidden layers, inputsize=28*28, output\n","class MLP(nn.Module):\n","  '''\n","  Fully connected neural network with 3 hidden layers.\n","  Input size: 784\n","  Output size: 10\n","  '''\n","  def __init__(self):\n","    super().__init__()\n","    self.flat = nn.Flatten()\n","    self.h1 = nn.Linear(28*28, 16)\n","    self.h2 = nn.Linear(16, 32)\n","    self.h3 = nn.Linear(32, 24)\n","    self.out = nn.Linear(24, 10)\n","    \n","  def forward(self, X, activ_hidden=nn.functional.relu):\n","    out = self.flat(X)\n","    out = activ_hidden(self.h1(out))\n","    out = activ_hidden(self.h2(out))\n","    out = activ_hidden(self.h3(out))\n","    out = self.out(out)\n","    return out"],"execution_count":3,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"3VN1NJXPm_rk","executionInfo":{"status":"ok","timestamp":1618147653666,"user_tz":-120,"elapsed":21264,"user":{"displayName":"Roberto Corti","photoUrl":"https://lh4.googleusercontent.com/-KaiJHEP6Eps/AAAAAAAAAAI/AAAAAAAAABU/y4Yv-P85NuU/s64/photo.jpg","userId":"08099351609548175475"}},"outputId":"3443f2d2-08e1-4413-bbc4-d61538b2f8df"},"source":["# https://stackoverflow.com/questions/66646604/http-error-503-service-unavailable-whan-trying-to-download-mnist-data\n","!wget www.di.ens.fr/~lelarge/MNIST.tar.gz\n","!tar -zxvf MNIST.tar.gz\n","\n","\n","# Device configuration\n","device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","\n","# Hyper-parameters \n","minibatch_size_train = 256\n","minibatch_size_test = 512\n","learning_rate = 0.01\n","\n","# Trasformations\n","transforms = Compose([\n","                      ToTensor(),\n","                      Normalize((0.1307,), (0.3081,))\n","                    ])\n","\n","# MNIST dataset \n","train_dataset = torchvision.datasets.MNIST(root='./data', \n","                                           train=True, \n","                                           transform=transforms,  \n","                                           download=True)\n","\n","test_dataset = torchvision.datasets.MNIST(root='./data', \n","                                          train=False, \n","                                          transform=transforms)\n","\n","\n","\n","# Data loader\n","train_loader = torch.utils.data.DataLoader(dataset=train_dataset, \n","                                           batch_size=minibatch_size_train, \n","                                           shuffle=True)\n","\n","test_loader = torch.utils.data.DataLoader(dataset=test_dataset, \n","                                          batch_size=minibatch_size_test, \n","                                          shuffle=False)\n"],"execution_count":4,"outputs":[{"output_type":"stream","text":["--2021-04-11 13:27:10--  http://www.di.ens.fr/~lelarge/MNIST.tar.gz\n","Resolving www.di.ens.fr (www.di.ens.fr)... 129.199.99.14\n","Connecting to www.di.ens.fr (www.di.ens.fr)|129.199.99.14|:80... connected.\n","HTTP request sent, awaiting response... 302 Found\n","Location: https://www.di.ens.fr/~lelarge/MNIST.tar.gz [following]\n","--2021-04-11 13:27:10--  https://www.di.ens.fr/~lelarge/MNIST.tar.gz\n","Connecting to www.di.ens.fr (www.di.ens.fr)|129.199.99.14|:443... connected.\n","HTTP request sent, awaiting response... 200 OK\n","Length: unspecified [application/x-gzip]\n","Saving to: ‘MNIST.tar.gz.4’\n","\n","MNIST.tar.gz.4          [               <=>  ]  33.20M  3.55MB/s    in 9.8s    \n","\n","2021-04-11 13:27:20 (3.38 MB/s) - ‘MNIST.tar.gz.4’ saved [34813078]\n","\n","MNIST/\n","MNIST/raw/\n","MNIST/raw/train-labels-idx1-ubyte\n","MNIST/raw/t10k-labels-idx1-ubyte.gz\n","MNIST/raw/t10k-labels-idx1-ubyte\n","MNIST/raw/t10k-images-idx3-ubyte.gz\n","MNIST/raw/train-images-idx3-ubyte\n","MNIST/raw/train-labels-idx1-ubyte.gz\n","MNIST/raw/t10k-images-idx3-ubyte\n","MNIST/raw/train-images-idx3-ubyte.gz\n","MNIST/processed/\n","MNIST/processed/training.pt\n","MNIST/processed/test.pt\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":267},"id":"DMiDnjb1p9JD","executionInfo":{"status":"ok","timestamp":1618147655948,"user_tz":-120,"elapsed":1567,"user":{"displayName":"Roberto Corti","photoUrl":"https://lh4.googleusercontent.com/-KaiJHEP6Eps/AAAAAAAAAAI/AAAAAAAAABU/y4Yv-P85NuU/s64/photo.jpg","userId":"08099351609548175475"}},"outputId":"d321a45e-e762-4372-8288-597c5a1f5562"},"source":["examples = iter(test_loader)\n","example_data, example_targets = examples.next()\n","\n","for i in range(6):\n","    plt.subplot(2,3,i+1)\n","    plt.imshow(example_data[i][0], cmap='gray')\n","plt.show()"],"execution_count":5,"outputs":[{"output_type":"display_data","data":{"image/png":"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\n","text/plain":["<Figure size 432x288 with 6 Axes>"]},"metadata":{"tags":[],"needs_background":"light"}}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"iO2kx70WmW82","executionInfo":{"status":"ok","timestamp":1618148053932,"user_tz":-120,"elapsed":392353,"user":{"displayName":"Roberto Corti","photoUrl":"https://lh4.googleusercontent.com/-KaiJHEP6Eps/AAAAAAAAAAI/AAAAAAAAABU/y4Yv-P85NuU/s64/photo.jpg","userId":"08099351609548175475"}},"outputId":"5ce5579e-8c70-4bdb-b0d5-a3fcaf998f69"},"source":["# Model\n","model = MLP().to(device)\n","\n","# Loss and optimizer\n","criterion = nn.CrossEntropyLoss()\n","optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)  \n","\n","# Number of epochs\n","num_epochs = 50\n","\n","# Train the model\n","n_total_steps = len(train_loader)\n","for epoch in range(num_epochs):\n","    for i, (images, labels) in enumerate(train_loader):  \n","        # origin shape: [100, 1, 28, 28]\n","        # resized: [100, 784]\n","        images = images.reshape(-1, 28*28).to(device)\n","        labels = labels.to(device)\n","        \n","        # Forward pass\n","        outputs = model(images)\n","        loss = criterion(outputs, labels)\n","\n","        # Adding L1 penalty\n","        l1_lambda = 0.001\n","        l1_norm = sum(p.abs().sum() \n","                         for p in model.parameters())\n","        loss = loss + l1_lambda * l1_norm\n","        \n","        # Backward and optimize\n","        optimizer.zero_grad()\n","        loss.backward()\n","        optimizer.step()\n","        \n","        if (i+1) % 100 == 0:\n","            print (f'Epoch [{epoch+1}/{num_epochs}], Step [{i+1}/{n_total_steps}], Loss: {loss.item():.4f}')\n","\n","\n","# Test the model\n","# In test phase, we don't need to compute gradients (for memory efficiency)\n","with torch.no_grad():\n","    n_correct = 0\n","    n_samples = 0\n","    for images, labels in test_loader:\n","        images = images.to(device)\n","        labels = labels.to(device)\n","        outputs = model(images)\n","        # max returns (value ,index)\n","        _, predicted = torch.max(outputs.data, 1)\n","        n_samples += labels.size(0)\n","        n_correct += (predicted == labels).sum().item()\n","\n","    acc = 100.0 * n_correct / n_samples\n","    print(f'Accuracy of the network on the 10000 test images: {acc} %')\n"],"execution_count":6,"outputs":[{"output_type":"stream","text":["Epoch [1/50], Step [100/235], Loss: 0.7075\n","Epoch [1/50], Step [200/235], Loss: 0.6002\n","Epoch [2/50], Step [100/235], Loss: 0.6411\n","Epoch [2/50], Step [200/235], Loss: 0.6248\n","Epoch [3/50], Step [100/235], Loss: 0.6721\n","Epoch [3/50], Step [200/235], Loss: 0.5890\n","Epoch [4/50], Step [100/235], Loss: 0.5726\n","Epoch [4/50], Step [200/235], Loss: 0.5697\n","Epoch [5/50], Step [100/235], Loss: 0.5422\n","Epoch [5/50], Step [200/235], Loss: 0.6351\n","Epoch [6/50], Step [100/235], Loss: 0.5743\n","Epoch [6/50], Step [200/235], Loss: 0.5759\n","Epoch [7/50], Step [100/235], Loss: 0.6375\n","Epoch [7/50], Step [200/235], Loss: 0.5507\n","Epoch [8/50], Step [100/235], Loss: 0.6872\n","Epoch [8/50], Step [200/235], Loss: 0.5670\n","Epoch [9/50], Step [100/235], Loss: 0.5825\n","Epoch [9/50], Step [200/235], Loss: 0.5099\n","Epoch [10/50], Step [100/235], Loss: 0.6691\n","Epoch [10/50], Step [200/235], Loss: 0.5979\n","Epoch [11/50], Step [100/235], Loss: 0.6217\n","Epoch [11/50], Step [200/235], Loss: 0.6035\n","Epoch [12/50], Step [100/235], Loss: 0.4677\n","Epoch [12/50], Step [200/235], Loss: 0.6361\n","Epoch [13/50], Step [100/235], Loss: 0.5100\n","Epoch [13/50], Step [200/235], Loss: 0.6621\n","Epoch [14/50], Step [100/235], Loss: 0.5745\n","Epoch [14/50], Step [200/235], Loss: 0.5620\n","Epoch [15/50], Step [100/235], Loss: 0.5214\n","Epoch [15/50], Step [200/235], Loss: 0.5330\n","Epoch [16/50], Step [100/235], Loss: 0.5162\n","Epoch [16/50], Step [200/235], Loss: 0.5190\n","Epoch [17/50], Step [100/235], Loss: 0.6355\n","Epoch [17/50], Step [200/235], Loss: 0.5305\n","Epoch [18/50], Step [100/235], Loss: 0.5607\n","Epoch [18/50], Step [200/235], Loss: 0.5231\n","Epoch [19/50], Step [100/235], Loss: 0.4901\n","Epoch [19/50], Step [200/235], Loss: 0.4538\n","Epoch [20/50], Step [100/235], Loss: 0.4997\n","Epoch [20/50], Step [200/235], Loss: 0.5118\n","Epoch [21/50], Step [100/235], Loss: 0.5678\n","Epoch [21/50], Step [200/235], Loss: 0.4951\n","Epoch [22/50], Step [100/235], Loss: 0.5311\n","Epoch [22/50], Step [200/235], Loss: 0.5038\n","Epoch [23/50], Step [100/235], Loss: 0.6086\n","Epoch [23/50], Step [200/235], Loss: 0.4637\n","Epoch [24/50], Step [100/235], Loss: 0.4870\n","Epoch [24/50], Step [200/235], Loss: 0.4794\n","Epoch [25/50], Step [100/235], Loss: 0.5493\n","Epoch [25/50], Step [200/235], Loss: 0.4978\n","Epoch [26/50], Step [100/235], Loss: 0.4648\n","Epoch [26/50], Step [200/235], Loss: 0.4817\n","Epoch [27/50], Step [100/235], Loss: 0.5568\n","Epoch [27/50], Step [200/235], Loss: 0.5086\n","Epoch [28/50], Step [100/235], Loss: 0.5847\n","Epoch [28/50], Step [200/235], Loss: 0.5260\n","Epoch [29/50], Step [100/235], Loss: 0.4248\n","Epoch [29/50], Step [200/235], Loss: 0.5683\n","Epoch [30/50], Step [100/235], Loss: 0.5717\n","Epoch [30/50], Step [200/235], Loss: 0.4270\n","Epoch [31/50], Step [100/235], Loss: 0.5361\n","Epoch [31/50], Step [200/235], Loss: 0.4755\n","Epoch [32/50], Step [100/235], Loss: 0.5696\n","Epoch [32/50], Step [200/235], Loss: 0.5191\n","Epoch [33/50], Step [100/235], Loss: 0.5532\n","Epoch [33/50], Step [200/235], Loss: 0.5646\n","Epoch [34/50], Step [100/235], Loss: 0.4928\n","Epoch [34/50], Step [200/235], Loss: 0.4500\n","Epoch [35/50], Step [100/235], Loss: 0.6081\n","Epoch [35/50], Step [200/235], Loss: 0.4592\n","Epoch [36/50], Step [100/235], Loss: 0.4894\n","Epoch [36/50], Step [200/235], Loss: 0.5281\n","Epoch [37/50], Step [100/235], Loss: 0.5566\n","Epoch [37/50], Step [200/235], Loss: 0.4905\n","Epoch [38/50], Step [100/235], Loss: 0.5041\n","Epoch [38/50], Step [200/235], Loss: 0.5369\n","Epoch [39/50], Step [100/235], Loss: 0.4935\n","Epoch [39/50], Step [200/235], Loss: 0.5690\n","Epoch [40/50], Step [100/235], Loss: 0.5116\n","Epoch [40/50], Step [200/235], Loss: 0.4914\n","Epoch [41/50], Step [100/235], Loss: 0.5130\n","Epoch [41/50], Step [200/235], Loss: 0.6025\n","Epoch [42/50], Step [100/235], Loss: 0.4918\n","Epoch [42/50], Step [200/235], Loss: 0.5372\n","Epoch [43/50], Step [100/235], Loss: 0.4889\n","Epoch [43/50], Step [200/235], Loss: 0.5060\n","Epoch [44/50], Step [100/235], Loss: 0.5160\n","Epoch [44/50], Step [200/235], Loss: 0.5571\n","Epoch [45/50], Step [100/235], Loss: 0.5403\n","Epoch [45/50], Step [200/235], Loss: 0.5237\n","Epoch [46/50], Step [100/235], Loss: 0.4478\n","Epoch [46/50], Step [200/235], Loss: 0.5224\n","Epoch [47/50], Step [100/235], Loss: 0.5154\n","Epoch [47/50], Step [200/235], Loss: 0.4556\n","Epoch [48/50], Step [100/235], Loss: 0.4930\n","Epoch [48/50], Step [200/235], Loss: 0.5281\n","Epoch [49/50], Step [100/235], Loss: 0.5533\n","Epoch [49/50], Step [200/235], Loss: 0.6050\n","Epoch [50/50], Step [100/235], Loss: 0.5370\n","Epoch [50/50], Step [200/235], Loss: 0.4577\n","Accuracy of the network on the 10000 test images: 93.99 %\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"ZHujtTGsucau"},"source":["### 2. Implement early stopping in the $E_{\\text{opt}}$ specification\n","\n","\n","To describe the criteria, let $E$ be the loss function of the training algorithm and let $E_{val}(t)$ the validation error at epoch $t$. The value $E_{opt}(t)$ is defined to be the lowest validation set error obtained up to $t$.:\n","\n","$$ E_{opt}(t) := \\min_{t' \\le t} E_{val}(t')$$\n","\n","The generalization loss at epoch $t$ the relative increase of the validation error over the minimum so-far:\n","\n","$$ GL(t) := 100 \\cdot \\biggl( \\frac{E_{val}(t)}{E_{opt}(t)} -1 \\biggr)$$\n","\n","The first class of stopping criteria defined in [Prechelt, L. (1998)](https://page.mi.fu-berlin.de/prechelt/Biblio/stop_tricks1997.pdf) is the following:\n","\n","$$ GL_{\\alpha}: \\text{stop training at first epoch for which} \\space GL(t) > \\alpha $$\n","\n","that is, stop as soon as the generalization loss exceeds a certain threshold."]},{"cell_type":"markdown","metadata":{"id":"QOJrEZVhwqLh"},"source":["In order to have this criteria we need to evaluate $E_{val}(t)$ at each epoch of the training:"]},{"cell_type":"code","metadata":{"id":"onhEPGqTnQXK"},"source":["def validation_error(test_loader, model, criterion, test_batch_size, device):\n","  with torch.no_grad():\n","    loss = 0\n","    for x_test, y_test in test_loader:\n","      x_test = x_test.to(device)\n","      y_test = y_test.to(device)\n","      y_hat = model(x_test)\n","      loss_batch = criterion(y_hat, y_test)\n","      loss += loss_batch\n","\n","    return loss.item() / len(test_loader)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"iHFOOd7Xw0IH"},"source":["Once provided such function the criteria can be implemented in our training loop by adding these following steps:\n","\n","```python\n","for epoch in range(num_epochs):\n","\n","  # training step\n","\n","  # evaluate E_val(epoch)\n","\n","  # evaluate E_opt(epoch)\n","\n","  # evaluate GL(epoch)\n","\n","  # check stop criteria \n","```\n"]},{"cell_type":"code","metadata":{"id":"IRoefcXgz8rI","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1617301935353,"user_tz":-120,"elapsed":21563,"user":{"displayName":"Roberto Corti","photoUrl":"https://lh4.googleusercontent.com/-KaiJHEP6Eps/AAAAAAAAAAI/AAAAAAAAABU/y4Yv-P85NuU/s64/photo.jpg","userId":"08099351609548175475"}},"outputId":"e6cc9d18-bcdd-45b9-cb55-fe1dfe1e0a25"},"source":["# Model\n","model = MLP().to(device)\n","\n","# Loss and optimizer\n","criterion = nn.CrossEntropyLoss()\n","optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)  \n","\n","# Number of epochs\n","num_epochs = 50\n","\n","# initialize E_opt\n","E_opt = np.inf\n","\n","# early stopping parameter\n","alpha = 5\n","\n","# Train the model\n","n_total_steps = len(train_loader)\n","for epoch in range(num_epochs):\n","    for i, (images, labels) in enumerate(train_loader):  \n","        # origin shape: [100, 1, 28, 28]\n","        # resized: [100, 784]\n","        images = images.reshape(-1, 28*28).to(device)\n","        labels = labels.to(device)\n","        \n","        # Forward pass\n","        outputs = model(images)\n","        loss = criterion(outputs, labels)\n","        \n","        # Backward and optimize\n","        optimizer.zero_grad()\n","        loss.backward()\n","        optimizer.step()\n","        \n","        if (i+1) == minibatch_size_train // 2:\n","            print(f'Epoch [{epoch+1}/{num_epochs}], Step [{i+1}/{n_total_steps}], Loss: {loss.item():.4f}')\n","\n","    # evaluate E_val (epoch)\n","    E_val = validation_error(test_loader, model, criterion, minibatch_size_test, device)\n","\n","    # evaluate E_opt (epoch)\n","    if E_opt > E_val:\n","      E_opt = E_val\n","\n","    # compute GL (epoch)\n","    GL = 100 * ( (E_val/E_opt)-1)\n","\n","    print(f'Epoch [{epoch+1}/{num_epochs}], E_val: {E_val:.4f}, E_opt: {E_opt:.4f}, GL: {GL:.3f}')\n","\n","    # apply criterion GL_alpha\n","    if GL > alpha:\n","      break"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Epoch [1/50], Step [128/235], Loss: 0.3249\n","Epoch [1/50], E_val: 0.2128, E_opt: 0.2128, GL: 0.000\n","Epoch [2/50], Step [128/235], Loss: 0.2114\n","Epoch [2/50], E_val: 0.2308, E_opt: 0.2128, GL: 8.433\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"kh_8DuCCsGnB","executionInfo":{"status":"ok","timestamp":1617301943565,"user_tz":-120,"elapsed":1761,"user":{"displayName":"Roberto Corti","photoUrl":"https://lh4.googleusercontent.com/-KaiJHEP6Eps/AAAAAAAAAAI/AAAAAAAAABU/y4Yv-P85NuU/s64/photo.jpg","userId":"08099351609548175475"}},"outputId":"43d76498-ffd6-4ce4-aaec-f66236d885bc"},"source":["with torch.no_grad():\n","    n_correct = 0\n","    n_samples = 0\n","    for images, labels in test_loader:\n","        images = images.to(device)\n","        labels = labels.to(device)\n","        outputs = model(images)\n","        # max returns (value ,index)\n","        _, predicted = torch.max(outputs.data, 1)\n","        n_samples += labels.size(0)\n","        n_correct += (predicted == labels).sum().item()\n","\n","    acc = 100.0 * n_correct / n_samples\n","    print(f'Accuracy of the network on the 10000 test images: {acc} %')"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Accuracy of the network on the 10000 test images: 93.13 %\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"8VJWS4JAex6N"},"source":["### 3. Try implementing Correct Class Quadratic Loss (CCQL) ([Demirkaya et al.](https://intra.ece.ucr.edu/~oymak/multiclass.pdf) ) in PyTorch"]},{"cell_type":"markdown","metadata":{"id":"oygwKA0eE7fG"},"source":["Given an input $\\bf{x}$, the model prediction $f(\\bf{x})$ the Correct-Class Quadratic-Loss (CCQL) is defined as\n","\n","$$ \\mathcal{L}_{CCQL}(f, \\mathcal{D}) = \\mathcal{L}_{QL}(f, \\mathcal{D}) + \\frac{w}{2} \\mathbb{E} [(1-f(\\mathbf{x})^2] $$\n","\n","where $\\mathcal{L}_{QL}(f, \\mathcal{D})$ is the quadratic loss."]},{"cell_type":"markdown","metadata":{"id":"mMom6POwJbuJ"},"source":["As suggested in the paper, I decide to pick $w = \\sqrt{K-1} - 1$, where $K$ specifies the number of classes. "]},{"cell_type":"code","metadata":{"id":"WXuyFNZ9PHKf","executionInfo":{"status":"ok","timestamp":1618151245884,"user_tz":-120,"elapsed":715,"user":{"displayName":"Roberto Corti","photoUrl":"https://lh4.googleusercontent.com/-KaiJHEP6Eps/AAAAAAAAAAI/AAAAAAAAABU/y4Yv-P85NuU/s64/photo.jpg","userId":"08099351609548175475"}}},"source":["class CCQLOSS(nn.Module):\n","  def __init__(self, weight=None, size_average=True):\n","    super(CCQLOSS, self).__init__()\n","\n","  def forward(self, outputs, labels, num_classes):\n","    '''\n","    Computes Correct Class Quadratic Loss.\n","    outputs: output of the model on the observations\n","    labels: expected output labels of the observations\n","    num_classes: number of classes of the data\n","    '''\n","    outputs=torch.nn.functional.softmax(outputs, dim=1)\n","    one_hot_labels = nn.functional.one_hot(labels).float()\n","    QLoss = nn.MSELoss()\n","    QL = QLoss(outputs, one_hot_labels)\n","    w = np.sqrt(num_classes-1)-1\n","    CTerm = (w/2) * torch.tensor([(1-outputs[i, labels[i]])**2 for i in range(0, len(labels))]).mean()\n","    return QL + CTerm"],"execution_count":77,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"njLEABwcJ9_x","executionInfo":{"status":"ok","timestamp":1618151787642,"user_tz":-120,"elapsed":540973,"user":{"displayName":"Roberto Corti","photoUrl":"https://lh4.googleusercontent.com/-KaiJHEP6Eps/AAAAAAAAAAI/AAAAAAAAABU/y4Yv-P85NuU/s64/photo.jpg","userId":"08099351609548175475"}},"outputId":"0a71e39d-8aa0-49da-d862-0a1bb6df6c39"},"source":["# Number of classes\n","num_classes = len(train_dataset.classes)\n","# Model\n","model = MLP().to(device)\n","\n","# Loss and optimizer\n","criterion = CCQLOSS()\n","optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)  \n","\n","# Number of epochs\n","num_epochs = 50\n","\n","# Train the model\n","n_total_steps = len(train_loader)\n","for epoch in range(num_epochs):\n","    for i, (images, labels) in enumerate(train_loader):  \n","        # origin shape: [100, 1, 28, 28]\n","        # resized: [100, 784]\n","        images = images.reshape(-1, 28*28).to(device)\n","        labels = labels.to(device)\n","        \n","        # Forward pass\n","        outputs = model(images)\n","        loss = criterion(outputs, labels, num_classes)\n","\n","        # Backward and optimize\n","        optimizer.zero_grad()\n","        loss.backward()\n","        optimizer.step()\n","        \n","        if (i+1) % 100 == 0:\n","            print (f'Epoch [{epoch+1}/{num_epochs}], Step [{i+1}/{n_total_steps}], Loss: {loss.item():.4f}')\n","\n","\n","# Test the model\n","# In test phase, we don't need to compute gradients (for memory efficiency)\n","with torch.no_grad():\n","    n_correct = 0\n","    n_samples = 0\n","    for images, labels in test_loader:\n","        images = images.to(device)\n","        labels = labels.to(device)\n","        outputs = model(images)\n","        # max returns (value ,index)\n","        _, predicted = torch.max(outputs.data, 1)\n","        n_samples += labels.size(0)\n","        n_correct += (predicted == labels).sum().item()\n","\n","    acc = 100.0 * n_correct / n_samples\n","    print(f'Accuracy of the network on the 10000 test images: {acc} %')\n"],"execution_count":78,"outputs":[{"output_type":"stream","text":["Epoch [1/50], Step [100/235], Loss: 0.1597\n","Epoch [1/50], Step [200/235], Loss: 0.0955\n","Epoch [2/50], Step [100/235], Loss: 0.0787\n","Epoch [2/50], Step [200/235], Loss: 0.0565\n","Epoch [3/50], Step [100/235], Loss: 0.0669\n","Epoch [3/50], Step [200/235], Loss: 0.0634\n","Epoch [4/50], Step [100/235], Loss: 0.0621\n","Epoch [4/50], Step [200/235], Loss: 0.0877\n","Epoch [5/50], Step [100/235], Loss: 0.0501\n","Epoch [5/50], Step [200/235], Loss: 0.0631\n","Epoch [6/50], Step [100/235], Loss: 0.0541\n","Epoch [6/50], Step [200/235], Loss: 0.0663\n","Epoch [7/50], Step [100/235], Loss: 0.0461\n","Epoch [7/50], Step [200/235], Loss: 0.0540\n","Epoch [8/50], Step [100/235], Loss: 0.0601\n","Epoch [8/50], Step [200/235], Loss: 0.0693\n","Epoch [9/50], Step [100/235], Loss: 0.0296\n","Epoch [9/50], Step [200/235], Loss: 0.0688\n","Epoch [10/50], Step [100/235], Loss: 0.0631\n","Epoch [10/50], Step [200/235], Loss: 0.0859\n","Epoch [11/50], Step [100/235], Loss: 0.0660\n","Epoch [11/50], Step [200/235], Loss: 0.0472\n","Epoch [12/50], Step [100/235], Loss: 0.0664\n","Epoch [12/50], Step [200/235], Loss: 0.0412\n","Epoch [13/50], Step [100/235], Loss: 0.0460\n","Epoch [13/50], Step [200/235], Loss: 0.0625\n","Epoch [14/50], Step [100/235], Loss: 0.0377\n","Epoch [14/50], Step [200/235], Loss: 0.0796\n","Epoch [15/50], Step [100/235], Loss: 0.0666\n","Epoch [15/50], Step [200/235], Loss: 0.0316\n","Epoch [16/50], Step [100/235], Loss: 0.0632\n","Epoch [16/50], Step [200/235], Loss: 0.0528\n","Epoch [17/50], Step [100/235], Loss: 0.0368\n","Epoch [17/50], Step [200/235], Loss: 0.0514\n","Epoch [18/50], Step [100/235], Loss: 0.0618\n","Epoch [18/50], Step [200/235], Loss: 0.0553\n","Epoch [19/50], Step [100/235], Loss: 0.0575\n","Epoch [19/50], Step [200/235], Loss: 0.0585\n","Epoch [20/50], Step [100/235], Loss: 0.0539\n","Epoch [20/50], Step [200/235], Loss: 0.0289\n","Epoch [21/50], Step [100/235], Loss: 0.0562\n","Epoch [21/50], Step [200/235], Loss: 0.0245\n","Epoch [22/50], Step [100/235], Loss: 0.0292\n","Epoch [22/50], Step [200/235], Loss: 0.0526\n","Epoch [23/50], Step [100/235], Loss: 0.0227\n","Epoch [23/50], Step [200/235], Loss: 0.0450\n","Epoch [24/50], Step [100/235], Loss: 0.0530\n","Epoch [24/50], Step [200/235], Loss: 0.0746\n","Epoch [25/50], Step [100/235], Loss: 0.0617\n","Epoch [25/50], Step [200/235], Loss: 0.0583\n","Epoch [26/50], Step [100/235], Loss: 0.0524\n","Epoch [26/50], Step [200/235], Loss: 0.0818\n","Epoch [27/50], Step [100/235], Loss: 0.0328\n","Epoch [27/50], Step [200/235], Loss: 0.0468\n","Epoch [28/50], Step [100/235], Loss: 0.0444\n","Epoch [28/50], Step [200/235], Loss: 0.0927\n","Epoch [29/50], Step [100/235], Loss: 0.0564\n","Epoch [29/50], Step [200/235], Loss: 0.0765\n","Epoch [30/50], Step [100/235], Loss: 0.1030\n","Epoch [30/50], Step [200/235], Loss: 0.0483\n","Epoch [31/50], Step [100/235], Loss: 0.0426\n","Epoch [31/50], Step [200/235], Loss: 0.0587\n","Epoch [32/50], Step [100/235], Loss: 0.0871\n","Epoch [32/50], Step [200/235], Loss: 0.0610\n","Epoch [33/50], Step [100/235], Loss: 0.0620\n","Epoch [33/50], Step [200/235], Loss: 0.0885\n","Epoch [34/50], Step [100/235], Loss: 0.0789\n","Epoch [34/50], Step [200/235], Loss: 0.1129\n","Epoch [35/50], Step [100/235], Loss: 0.0674\n","Epoch [35/50], Step [200/235], Loss: 0.1733\n","Epoch [36/50], Step [100/235], Loss: 0.0814\n","Epoch [36/50], Step [200/235], Loss: 0.1350\n","Epoch [37/50], Step [100/235], Loss: 0.0864\n","Epoch [37/50], Step [200/235], Loss: 0.1048\n","Epoch [38/50], Step [100/235], Loss: 0.1085\n","Epoch [38/50], Step [200/235], Loss: 0.1020\n","Epoch [39/50], Step [100/235], Loss: 0.0888\n","Epoch [39/50], Step [200/235], Loss: 0.0955\n","Epoch [40/50], Step [100/235], Loss: 0.1165\n","Epoch [40/50], Step [200/235], Loss: 0.1491\n","Epoch [41/50], Step [100/235], Loss: 0.0844\n","Epoch [41/50], Step [200/235], Loss: 0.0969\n","Epoch [42/50], Step [100/235], Loss: 0.1031\n","Epoch [42/50], Step [200/235], Loss: 0.1049\n","Epoch [43/50], Step [100/235], Loss: 0.1005\n","Epoch [43/50], Step [200/235], Loss: 0.1241\n","Epoch [44/50], Step [100/235], Loss: 0.0983\n","Epoch [44/50], Step [200/235], Loss: 0.1659\n","Epoch [45/50], Step [100/235], Loss: 0.0814\n","Epoch [45/50], Step [200/235], Loss: 0.0816\n","Epoch [46/50], Step [100/235], Loss: 0.0532\n","Epoch [46/50], Step [200/235], Loss: 0.1244\n","Epoch [47/50], Step [100/235], Loss: 0.0750\n","Epoch [47/50], Step [200/235], Loss: 0.0683\n","Epoch [48/50], Step [100/235], Loss: 0.0687\n","Epoch [48/50], Step [200/235], Loss: 0.0902\n","Epoch [49/50], Step [100/235], Loss: 0.0984\n","Epoch [49/50], Step [200/235], Loss: 0.0902\n","Epoch [50/50], Step [100/235], Loss: 0.1313\n","Epoch [50/50], Step [200/235], Loss: 0.1008\n","Accuracy of the network on the 10000 test images: 84.87 %\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"GjNEmG0TUuKw"},"source":[""],"execution_count":null,"outputs":[]}]}