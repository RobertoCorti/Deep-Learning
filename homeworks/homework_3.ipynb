{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"homework_3.ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyOddAaf+p6HUYfhRlF0BNm0"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","metadata":{"id":"ZrkJTj_AeTpn"},"source":["# Deep Learning - Homework 03\n","\n","### Roberto Corti"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"6gtQvJH9nckF","executionInfo":{"status":"ok","timestamp":1617299618034,"user_tz":-120,"elapsed":29695,"user":{"displayName":"Roberto Corti","photoUrl":"https://lh4.googleusercontent.com/-KaiJHEP6Eps/AAAAAAAAAAI/AAAAAAAAABU/y4Yv-P85NuU/s64/photo.jpg","userId":"08099351609548175475"}},"outputId":"cadfe361-1ba4-4c3d-9b37-2afc20195150"},"source":["from google.colab import drive\n","\n","folder_mount = '/content/drive' # Your Drive will be mounted on top of this path\n","\n","drive.mount(folder_mount)\n","\n","%cd drive/MyDrive/Università/DSSC/Secondo\\ Anno/Deep-Learning/homeworks/"],"execution_count":1,"outputs":[{"output_type":"stream","text":["Mounted at /content/drive\n","/content/drive/MyDrive/Università/DSSC/Secondo Anno/Deep-Learning/homeworks\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"pT8WjCakfJ8r"},"source":["1. Implement L1 norm regularization as a custom loss function"]},{"cell_type":"code","metadata":{"id":"Vm9OATNuBUW3","executionInfo":{"status":"ok","timestamp":1617301357478,"user_tz":-120,"elapsed":945,"user":{"displayName":"Roberto Corti","photoUrl":"https://lh4.googleusercontent.com/-KaiJHEP6Eps/AAAAAAAAAAI/AAAAAAAAABU/y4Yv-P85NuU/s64/photo.jpg","userId":"08099351609548175475"}}},"source":["import torch\n","import torch.nn as nn\n","import torchvision\n","import numpy as np\n","from torchvision.datasets import MNIST\n","import matplotlib.pyplot as plt\n","from torchvision.transforms import ToTensor, Normalize, Compose"],"execution_count":35,"outputs":[]},{"cell_type":"code","metadata":{"id":"m2AtNjd6kuO9","executionInfo":{"status":"ok","timestamp":1617299630301,"user_tz":-120,"elapsed":1144,"user":{"displayName":"Roberto Corti","photoUrl":"https://lh4.googleusercontent.com/-KaiJHEP6Eps/AAAAAAAAAAI/AAAAAAAAABU/y4Yv-P85NuU/s64/photo.jpg","userId":"08099351609548175475"}}},"source":["# Fully connected neural network with 3 hidden layers, inputsize=28*28, output\n","class MLP(nn.Module):\n","  '''\n","  Fully connected neural network with 3 hidden layers.\n","  Input size: 784\n","  Output size: 10\n","  '''\n","  def __init__(self):\n","    super().__init__()\n","    self.flat = nn.Flatten()\n","    self.h1 = nn.Linear(28*28, 16)\n","    self.h2 = nn.Linear(16, 32)\n","    self.h3 = nn.Linear(32, 24)\n","    self.out = nn.Linear(24, 10)\n","    \n","  def forward(self, X, activ_hidden=nn.functional.relu):\n","    out = self.flat(X)\n","    out = activ_hidden(self.h1(out))\n","    out = activ_hidden(self.h2(out))\n","    out = activ_hidden(self.h3(out))\n","    out = self.out(out)\n","    return out"],"execution_count":3,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"3VN1NJXPm_rk","executionInfo":{"status":"ok","timestamp":1617299665191,"user_tz":-120,"elapsed":29626,"user":{"displayName":"Roberto Corti","photoUrl":"https://lh4.googleusercontent.com/-KaiJHEP6Eps/AAAAAAAAAAI/AAAAAAAAABU/y4Yv-P85NuU/s64/photo.jpg","userId":"08099351609548175475"}},"outputId":"5d4518ca-bb35-4f69-b8f5-70110d0a15c0"},"source":["# https://stackoverflow.com/questions/66646604/http-error-503-service-unavailable-whan-trying-to-download-mnist-data\n","!wget www.di.ens.fr/~lelarge/MNIST.tar.gz\n","!tar -zxvf MNIST.tar.gz\n","\n","\n","# Device configuration\n","device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","\n","# Hyper-parameters \n","minibatch_size_train = 256\n","minibatch_size_test = 512\n","learning_rate = 0.01\n","\n","# Trasformations\n","transforms = Compose([\n","                      ToTensor(),\n","                      Normalize((0.1307,), (0.3081,))\n","                    ])\n","\n","# MNIST dataset \n","train_dataset = torchvision.datasets.MNIST(root='./data', \n","                                           train=True, \n","                                           transform=transforms,  \n","                                           download=True)\n","\n","test_dataset = torchvision.datasets.MNIST(root='./data', \n","                                          train=False, \n","                                          transform=transforms)\n","\n","\n","\n","# Data loader\n","train_loader = torch.utils.data.DataLoader(dataset=train_dataset, \n","                                           batch_size=minibatch_size_train, \n","                                           shuffle=True)\n","\n","test_loader = torch.utils.data.DataLoader(dataset=test_dataset, \n","                                          batch_size=minibatch_size_test, \n","                                          shuffle=False)\n"],"execution_count":4,"outputs":[{"output_type":"stream","text":["--2021-04-01 17:53:56--  http://www.di.ens.fr/~lelarge/MNIST.tar.gz\n","Resolving www.di.ens.fr (www.di.ens.fr)... 129.199.99.14\n","Connecting to www.di.ens.fr (www.di.ens.fr)|129.199.99.14|:80... connected.\n","HTTP request sent, awaiting response... 302 Found\n","Location: https://www.di.ens.fr/~lelarge/MNIST.tar.gz [following]\n","--2021-04-01 17:53:56--  https://www.di.ens.fr/~lelarge/MNIST.tar.gz\n","Connecting to www.di.ens.fr (www.di.ens.fr)|129.199.99.14|:443... connected.\n","HTTP request sent, awaiting response... 200 OK\n","Length: unspecified [application/x-gzip]\n","Saving to: ‘MNIST.tar.gz.2’\n","\n","MNIST.tar.gz.2          [           <=>      ]  33.20M  2.80MB/s    in 21s     \n","\n","2021-04-01 17:54:17 (1.60 MB/s) - ‘MNIST.tar.gz.2’ saved [34813078]\n","\n","MNIST/\n","MNIST/raw/\n","MNIST/raw/train-labels-idx1-ubyte\n","MNIST/raw/t10k-labels-idx1-ubyte.gz\n","MNIST/raw/t10k-labels-idx1-ubyte\n","MNIST/raw/t10k-images-idx3-ubyte.gz\n","MNIST/raw/train-images-idx3-ubyte\n","MNIST/raw/train-labels-idx1-ubyte.gz\n","MNIST/raw/t10k-images-idx3-ubyte\n","MNIST/raw/train-images-idx3-ubyte.gz\n","MNIST/processed/\n","MNIST/processed/training.pt\n","MNIST/processed/test.pt\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":267},"id":"DMiDnjb1p9JD","executionInfo":{"status":"ok","timestamp":1617299673060,"user_tz":-120,"elapsed":1796,"user":{"displayName":"Roberto Corti","photoUrl":"https://lh4.googleusercontent.com/-KaiJHEP6Eps/AAAAAAAAAAI/AAAAAAAAABU/y4Yv-P85NuU/s64/photo.jpg","userId":"08099351609548175475"}},"outputId":"b528e4e9-0d4c-41a9-cf13-5e17906cd23e"},"source":["examples = iter(test_loader)\n","example_data, example_targets = examples.next()\n","\n","for i in range(6):\n","    plt.subplot(2,3,i+1)\n","    plt.imshow(example_data[i][0], cmap='gray')\n","plt.show()\n"],"execution_count":5,"outputs":[{"output_type":"display_data","data":{"image/png":"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\n","text/plain":["<Figure size 432x288 with 6 Axes>"]},"metadata":{"tags":[],"needs_background":"light"}}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"iO2kx70WmW82","executionInfo":{"status":"ok","timestamp":1617300377419,"user_tz":-120,"elapsed":444913,"user":{"displayName":"Roberto Corti","photoUrl":"https://lh4.googleusercontent.com/-KaiJHEP6Eps/AAAAAAAAAAI/AAAAAAAAABU/y4Yv-P85NuU/s64/photo.jpg","userId":"08099351609548175475"}},"outputId":"0b0f5767-13d0-40d6-f5b5-1b440638a95b"},"source":["# Model\n","model = MLP().to(device)\n","\n","# Loss and optimizer\n","criterion = nn.CrossEntropyLoss()\n","optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)  \n","\n","# Number of epochs\n","num_epochs = 50\n","\n","# Train the model\n","n_total_steps = len(train_loader)\n","for epoch in range(num_epochs):\n","    for i, (images, labels) in enumerate(train_loader):  \n","        # origin shape: [100, 1, 28, 28]\n","        # resized: [100, 784]\n","        images = images.reshape(-1, 28*28).to(device)\n","        labels = labels.to(device)\n","        \n","        # Forward pass\n","        outputs = model(images)\n","        loss = criterion(outputs, labels)\n","\n","        # Adding L1 penalty\n","        l1_lambda = 0.001\n","        l1_norm = sum(p.abs().sum() \n","                         for p in model.parameters())\n","        loss = loss + l1_lambda * l1_norm\n","        \n","        # Backward and optimize\n","        optimizer.zero_grad()\n","        loss.backward()\n","        optimizer.step()\n","        \n","        if (i+1) % 100 == 0:\n","            print (f'Epoch [{epoch+1}/{num_epochs}], Step [{i+1}/{n_total_steps}], Loss: {loss.item():.4f}')\n","\n","\n","# Test the model\n","# In test phase, we don't need to compute gradients (for memory efficiency)\n","with torch.no_grad():\n","    n_correct = 0\n","    n_samples = 0\n","    for images, labels in test_loader:\n","        images = images.to(device)\n","        labels = labels.to(device)\n","        outputs = model(images)\n","        # max returns (value ,index)\n","        _, predicted = torch.max(outputs.data, 1)\n","        n_samples += labels.size(0)\n","        n_correct += (predicted == labels).sum().item()\n","\n","    acc = 100.0 * n_correct / n_samples\n","    print(f'Accuracy of the network on the 10000 test images: {acc} %')\n"],"execution_count":10,"outputs":[{"output_type":"stream","text":["Epoch [1/50], Step [100/235], Loss: 0.8294\n","Epoch [1/50], Step [200/235], Loss: 0.6611\n","Epoch [2/50], Step [100/235], Loss: 0.6915\n","Epoch [2/50], Step [200/235], Loss: 0.6201\n","Epoch [3/50], Step [100/235], Loss: 0.5455\n","Epoch [3/50], Step [200/235], Loss: 0.6633\n","Epoch [4/50], Step [100/235], Loss: 0.6622\n","Epoch [4/50], Step [200/235], Loss: 0.4757\n","Epoch [5/50], Step [100/235], Loss: 0.5511\n","Epoch [5/50], Step [200/235], Loss: 0.5219\n","Epoch [6/50], Step [100/235], Loss: 0.5359\n","Epoch [6/50], Step [200/235], Loss: 0.5743\n","Epoch [7/50], Step [100/235], Loss: 0.4834\n","Epoch [7/50], Step [200/235], Loss: 0.5680\n","Epoch [8/50], Step [100/235], Loss: 0.6402\n","Epoch [8/50], Step [200/235], Loss: 0.6160\n","Epoch [9/50], Step [100/235], Loss: 0.5505\n","Epoch [9/50], Step [200/235], Loss: 0.5024\n","Epoch [10/50], Step [100/235], Loss: 0.5944\n","Epoch [10/50], Step [200/235], Loss: 0.6014\n","Epoch [11/50], Step [100/235], Loss: 0.5729\n","Epoch [11/50], Step [200/235], Loss: 0.5812\n","Epoch [12/50], Step [100/235], Loss: 0.4794\n","Epoch [12/50], Step [200/235], Loss: 0.6436\n","Epoch [13/50], Step [100/235], Loss: 0.5678\n","Epoch [13/50], Step [200/235], Loss: 0.6541\n","Epoch [14/50], Step [100/235], Loss: 0.5547\n","Epoch [14/50], Step [200/235], Loss: 0.5667\n","Epoch [15/50], Step [100/235], Loss: 0.5290\n","Epoch [15/50], Step [200/235], Loss: 0.5425\n","Epoch [16/50], Step [100/235], Loss: 0.6018\n","Epoch [16/50], Step [200/235], Loss: 0.5399\n","Epoch [17/50], Step [100/235], Loss: 0.4783\n","Epoch [17/50], Step [200/235], Loss: 0.5087\n","Epoch [18/50], Step [100/235], Loss: 0.5539\n","Epoch [18/50], Step [200/235], Loss: 0.6691\n","Epoch [19/50], Step [100/235], Loss: 0.5431\n","Epoch [19/50], Step [200/235], Loss: 0.4909\n","Epoch [20/50], Step [100/235], Loss: 0.5507\n","Epoch [20/50], Step [200/235], Loss: 0.5165\n","Epoch [21/50], Step [100/235], Loss: 0.5082\n","Epoch [21/50], Step [200/235], Loss: 0.4803\n","Epoch [22/50], Step [100/235], Loss: 0.5689\n","Epoch [22/50], Step [200/235], Loss: 0.5033\n","Epoch [23/50], Step [100/235], Loss: 0.5353\n","Epoch [23/50], Step [200/235], Loss: 0.4752\n","Epoch [24/50], Step [100/235], Loss: 0.6155\n","Epoch [24/50], Step [200/235], Loss: 0.5131\n","Epoch [25/50], Step [100/235], Loss: 0.6032\n","Epoch [25/50], Step [200/235], Loss: 0.6044\n","Epoch [26/50], Step [100/235], Loss: 0.5269\n","Epoch [26/50], Step [200/235], Loss: 0.4404\n","Epoch [27/50], Step [100/235], Loss: 0.5680\n","Epoch [27/50], Step [200/235], Loss: 0.4983\n","Epoch [28/50], Step [100/235], Loss: 0.5460\n","Epoch [28/50], Step [200/235], Loss: 0.5825\n","Epoch [29/50], Step [100/235], Loss: 0.5493\n","Epoch [29/50], Step [200/235], Loss: 0.5368\n","Epoch [30/50], Step [100/235], Loss: 0.5410\n","Epoch [30/50], Step [200/235], Loss: 0.4944\n","Epoch [31/50], Step [100/235], Loss: 0.4588\n","Epoch [31/50], Step [200/235], Loss: 0.5090\n","Epoch [32/50], Step [100/235], Loss: 0.5379\n","Epoch [32/50], Step [200/235], Loss: 0.5768\n","Epoch [33/50], Step [100/235], Loss: 0.4748\n","Epoch [33/50], Step [200/235], Loss: 0.6332\n","Epoch [34/50], Step [100/235], Loss: 0.5220\n","Epoch [34/50], Step [200/235], Loss: 0.5175\n","Epoch [35/50], Step [100/235], Loss: 0.4996\n","Epoch [35/50], Step [200/235], Loss: 0.4821\n","Epoch [36/50], Step [100/235], Loss: 0.5335\n","Epoch [36/50], Step [200/235], Loss: 0.5147\n","Epoch [37/50], Step [100/235], Loss: 0.4796\n","Epoch [37/50], Step [200/235], Loss: 0.4738\n","Epoch [38/50], Step [100/235], Loss: 0.4102\n","Epoch [38/50], Step [200/235], Loss: 0.6249\n","Epoch [39/50], Step [100/235], Loss: 0.6704\n","Epoch [39/50], Step [200/235], Loss: 0.5710\n","Epoch [40/50], Step [100/235], Loss: 0.5586\n","Epoch [40/50], Step [200/235], Loss: 0.5604\n","Epoch [41/50], Step [100/235], Loss: 0.4342\n","Epoch [41/50], Step [200/235], Loss: 0.5451\n","Epoch [42/50], Step [100/235], Loss: 0.5530\n","Epoch [42/50], Step [200/235], Loss: 0.4718\n","Epoch [43/50], Step [100/235], Loss: 0.7025\n","Epoch [43/50], Step [200/235], Loss: 0.5943\n","Epoch [44/50], Step [100/235], Loss: 0.5939\n","Epoch [44/50], Step [200/235], Loss: 0.6348\n","Epoch [45/50], Step [100/235], Loss: 0.5107\n","Epoch [45/50], Step [200/235], Loss: 0.5916\n","Epoch [46/50], Step [100/235], Loss: 0.5182\n","Epoch [46/50], Step [200/235], Loss: 0.4946\n","Epoch [47/50], Step [100/235], Loss: 0.4913\n","Epoch [47/50], Step [200/235], Loss: 0.4458\n","Epoch [48/50], Step [100/235], Loss: 0.6389\n","Epoch [48/50], Step [200/235], Loss: 0.4705\n","Epoch [49/50], Step [100/235], Loss: 0.5549\n","Epoch [49/50], Step [200/235], Loss: 0.4714\n","Epoch [50/50], Step [100/235], Loss: 0.5200\n","Epoch [50/50], Step [200/235], Loss: 0.5309\n","Accuracy of the network on the 10000 test images: 93.82 %\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"ZHujtTGsucau"},"source":["### 2. Implement early stopping in the $E_{\\text{opt}}$ specification\n","\n","\n","To describe the criteria, let $E$ be the loss function of the training algorithm and let $E_{val}(t)$ the validation error at epoch $t$. The value $E_{opt}(t)$ is defined to be the lowest validation set error obtained up to $t$.:\n","\n","$$ E_{opt}(t) := \\min_{t' \\le t} E_{val}(t')$$\n","\n","The generalization loss at epoch $t$ the relative increase of the validation error over the minimum so-far:\n","\n","$$ GL(t) := 100 \\cdot \\biggl( \\frac{E_{val}(t)}{E_{opt}(t)} -1 \\biggr)$$\n","\n","The first class of stopping criteria defined in [Prechelt, L. (1998)](https://page.mi.fu-berlin.de/prechelt/Biblio/stop_tricks1997.pdf) is the following:\n","\n","$$ GL_{\\alpha}: \\text{stop training at first epoch for which} \\space GL(t) > \\alpha $$\n","\n","that is, stop as soon as the generalization loss exceeds a certain threshold."]},{"cell_type":"markdown","metadata":{"id":"QOJrEZVhwqLh"},"source":["In order to have this criteria we need to evaluate $E_val(t)$ at each epoch of the training:"]},{"cell_type":"code","metadata":{"id":"onhEPGqTnQXK","executionInfo":{"status":"ok","timestamp":1617301672613,"user_tz":-120,"elapsed":2122,"user":{"displayName":"Roberto Corti","photoUrl":"https://lh4.googleusercontent.com/-KaiJHEP6Eps/AAAAAAAAAAI/AAAAAAAAABU/y4Yv-P85NuU/s64/photo.jpg","userId":"08099351609548175475"}}},"source":["def validation_error(test_loader, model, criterion, test_batch_size, device):\n","  with torch.no_grad():\n","    loss = 0\n","    for x_test, y_test in test_loader:\n","      x_test = x_test.to(device)\n","      y_test = y_test.to(device)\n","      y_hat = model(x_test)\n","      loss_batch = criterion(y_hat, y_test)\n","      loss += loss_batch\n","\n","    return loss.item() / len(test_loader)"],"execution_count":41,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"iHFOOd7Xw0IH"},"source":["Once provided such function the criteria can be implemented in our training loop by adding these following steps:\n","\n","```python\n","for epoch in range(num_epochs):\n","\n","  # training step\n","\n","  # evaluate E_val(epoch)\n","\n","  # evaluate E_opt(epoch)\n","\n","  # evaluate GL(epoch)\n","\n","  # check stop criteria \n","```\n"]},{"cell_type":"code","metadata":{"id":"IRoefcXgz8rI","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1617301935353,"user_tz":-120,"elapsed":21563,"user":{"displayName":"Roberto Corti","photoUrl":"https://lh4.googleusercontent.com/-KaiJHEP6Eps/AAAAAAAAAAI/AAAAAAAAABU/y4Yv-P85NuU/s64/photo.jpg","userId":"08099351609548175475"}},"outputId":"e6cc9d18-bcdd-45b9-cb55-fe1dfe1e0a25"},"source":["# Model\n","model = MLP().to(device)\n","\n","# Loss and optimizer\n","criterion = nn.CrossEntropyLoss()\n","optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)  \n","\n","# Number of epochs\n","num_epochs = 50\n","\n","# initialize E_opt\n","E_opt = np.inf\n","\n","# early stopping parameter\n","alpha = 5\n","\n","# Train the model\n","n_total_steps = len(train_loader)\n","for epoch in range(num_epochs):\n","    for i, (images, labels) in enumerate(train_loader):  \n","        # origin shape: [100, 1, 28, 28]\n","        # resized: [100, 784]\n","        images = images.reshape(-1, 28*28).to(device)\n","        labels = labels.to(device)\n","        \n","        # Forward pass\n","        outputs = model(images)\n","        loss = criterion(outputs, labels)\n","        \n","        # Backward and optimize\n","        optimizer.zero_grad()\n","        loss.backward()\n","        optimizer.step()\n","        \n","        if (i+1) == minibatch_size_train // 2:\n","            print(f'Epoch [{epoch+1}/{num_epochs}], Step [{i+1}/{n_total_steps}], Loss: {loss.item():.4f}')\n","\n","    # evaluate E_val (epoch)\n","    E_val = validation_error(test_loader, model, criterion, minibatch_size_test, device)\n","\n","    # evaluate E_opt (epoch)\n","    if E_opt > E_val:\n","      E_opt = E_val\n","\n","    # compute GL (epoch)\n","    GL = 100 * ( (E_val/E_opt)-1)\n","\n","    print(f'Epoch [{epoch+1}/{num_epochs}], E_val: {E_val:.4f}, E_opt: {E_opt:.4f}, GL: {GL:.3f}')\n","\n","    # apply criterion GL_alpha\n","    if GL > alpha:\n","      break"],"execution_count":45,"outputs":[{"output_type":"stream","text":["Epoch [1/50], Step [128/235], Loss: 0.3249\n","Epoch [1/50], E_val: 0.2128, E_opt: 0.2128, GL: 0.000\n","Epoch [2/50], Step [128/235], Loss: 0.2114\n","Epoch [2/50], E_val: 0.2308, E_opt: 0.2128, GL: 8.433\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"kh_8DuCCsGnB","executionInfo":{"status":"ok","timestamp":1617301943565,"user_tz":-120,"elapsed":1761,"user":{"displayName":"Roberto Corti","photoUrl":"https://lh4.googleusercontent.com/-KaiJHEP6Eps/AAAAAAAAAAI/AAAAAAAAABU/y4Yv-P85NuU/s64/photo.jpg","userId":"08099351609548175475"}},"outputId":"43d76498-ffd6-4ce4-aaec-f66236d885bc"},"source":["with torch.no_grad():\n","    n_correct = 0\n","    n_samples = 0\n","    for images, labels in test_loader:\n","        images = images.to(device)\n","        labels = labels.to(device)\n","        outputs = model(images)\n","        # max returns (value ,index)\n","        _, predicted = torch.max(outputs.data, 1)\n","        n_samples += labels.size(0)\n","        n_correct += (predicted == labels).sum().item()\n","\n","    acc = 100.0 * n_correct / n_samples\n","    print(f'Accuracy of the network on the 10000 test images: {acc} %')"],"execution_count":46,"outputs":[{"output_type":"stream","text":["Accuracy of the network on the 10000 test images: 93.13 %\n"],"name":"stdout"}]}]}