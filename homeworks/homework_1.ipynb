{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"homework_1.ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyN3+Qh/IgyZrclzC5oHFM2V"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","metadata":{"id":"g7cp77x-Y3en"},"source":["# Deep Learning - Homework 01"]},{"cell_type":"markdown","metadata":{"id":"62AUzV_hz1dg"},"source":["### 1. Build the MLP using PT built-ins as in [lab-01](../labs/01-intro-to-pt.ipynb)\n","\n","Let's summarize the structure of the MLP model:\n","\n","\n","**Layer 1**:  \n","* $ z^{(1)} = W_1  x$,  where $x \\in  \\mathbb{R}^{5} $ and $W_1 \\in \\mathbb{R}^{11 \\times 5}$\n","* $ a^{(1)} = h^{(1)}(z^{(1)})$, where $h^{(1)}(x) = ReLU(x)$\n","\n","\n","**Layer 2**:  \n","* $ z^{(2)} = W_2  a^{(1)}$,  where $a^{(1)} \\in \\mathbb{R}^{11}$ and $W_2 \\in \\mathbb{R}^{16 \\times 11}$\n","* $ a^{(2)} = h^{(2)}(z^{(2)})$, where $h^{(2)}(x) = ReLU(x)$\n","\n","**Layer 3**:  \n","* $ z^{(3)} = W_3  a^{(2)}$,  where $a^{(2)} \\in \\mathbb{R}^{16}$ and $W_3 \\in \\mathbb{R}^{13 \\times 16}$\n","* $ a^{(3)} = h^{(3)}(z^{(3)})$, where $h^{(3)}(x) = ReLU(x)$\n","\n","\n","**Layer 4**:  \n","* $ z^{(4)} = W_4  a^{(3)}$,  where $a^{(3)} \\in \\mathbb{R}^{13}$ and $W_4 \\in \\mathbb{R}^{8 \\times 13}$\n","* $ a^{(4)} = h^{(4)}(z^{(4)})$, where $h^{(4)}(x) = ReLU(x)$\n","\n","**Layer 5**:  \n","* $ z^{(5)} = W_5  a^{(4)}$,  where $a^{(4)} \\in \\mathbb{R}^{8}$ and $W_5 \\in \\mathbb{R}^{4 \\times 8}$\n","* $ y = h^{(5)}(z^{(5)})$, where $h^{(5)}(x) = softmax(x)$"]},{"cell_type":"code","metadata":{"id":"cXuH2fuLZKNq","executionInfo":{"status":"ok","timestamp":1616246856840,"user_tz":-60,"elapsed":733,"user":{"displayName":"Roberto Corti","photoUrl":"https://lh4.googleusercontent.com/-KaiJHEP6Eps/AAAAAAAAAAI/AAAAAAAAABU/y4Yv-P85NuU/s64/photo.jpg","userId":"08099351609548175475"}}},"source":["import torch\n","\n","class MLP(torch.nn.Module):\n","  def __init__(self):\n","    super().__init__()\n","    self.layer1 = torch.nn.Linear(in_features=5, out_features=11,  bias=False)\n","    self.layer2 = torch.nn.Linear(in_features=11, out_features=16, bias=False)\n","    self.layer3 = torch.nn.Linear(in_features=16, out_features=13, bias=False)\n","    self.layer4 = torch.nn.Linear(in_features=13, out_features=8,  bias=False)\n","    self.outlayer = torch.nn.Linear(in_features=8, out_features=4,  bias=False)\n","    \n","  def forward(self, x):\n","    out = self.layer1(x)\n","    out = torch.nn.functional.relu(out)\n","    out = self.layer2(out)\n","    out = torch.nn.functional.relu(out)\n","    out = self.layer3(out)\n","    out = torch.nn.functional.relu(out)\n","    out = self.layer4(out)\n","    out = torch.nn.functional.relu(out)\n","    out = self.outlayer(out)\n","    out = torch.nn.functional.softmax(out)\n","    return out\n"],"execution_count":12,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"wMJ71skE4lBe"},"source":["### 2. Instantiate and summarise the MLP model built with PT"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"xJuUdMw9ZWwI","executionInfo":{"status":"ok","timestamp":1616247955001,"user_tz":-60,"elapsed":963,"user":{"displayName":"Roberto Corti","photoUrl":"https://lh4.googleusercontent.com/-KaiJHEP6Eps/AAAAAAAAAAI/AAAAAAAAABU/y4Yv-P85NuU/s64/photo.jpg","userId":"08099351609548175475"}},"outputId":"cce491f9-e14a-49c0-f1ad-dbb9b2aa3d2e"},"source":["model = MLP()\n","model"],"execution_count":13,"outputs":[{"output_type":"execute_result","data":{"text/plain":["MLP(\n","  (layer1): Linear(in_features=5, out_features=11, bias=False)\n","  (layer2): Linear(in_features=11, out_features=16, bias=False)\n","  (layer3): Linear(in_features=16, out_features=13, bias=False)\n","  (layer4): Linear(in_features=13, out_features=8, bias=False)\n","  (outlayer): Linear(in_features=8, out_features=4, bias=False)\n",")"]},"metadata":{"tags":[]},"execution_count":13}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ENVEzaRdZNQG","executionInfo":{"status":"ok","timestamp":1616246564485,"user_tz":-60,"elapsed":4688,"user":{"displayName":"Roberto Corti","photoUrl":"https://lh4.googleusercontent.com/-KaiJHEP6Eps/AAAAAAAAAAI/AAAAAAAAABU/y4Yv-P85NuU/s64/photo.jpg","userId":"08099351609548175475"}},"outputId":"e007771c-4370-4e8d-c8e4-51fe3dc268ad"},"source":["import sys\n","!{sys.executable} -m pip install torch-summary #how to use pip or conda in jupyter notebooks\n","from torchsummary import summary"],"execution_count":5,"outputs":[{"output_type":"stream","text":["Collecting torch-summary\n","  Downloading https://files.pythonhosted.org/packages/ca/db/93d18c84f73b214acfa4d18051d6f4263eee3e044c408928e8abe941a22c/torch_summary-1.4.5-py3-none-any.whl\n","Installing collected packages: torch-summary\n","Successfully installed torch-summary-1.4.5\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"QOw8uxvFZZoV","executionInfo":{"status":"ok","timestamp":1616246568320,"user_tz":-60,"elapsed":865,"user":{"displayName":"Roberto Corti","photoUrl":"https://lh4.googleusercontent.com/-KaiJHEP6Eps/AAAAAAAAAAI/AAAAAAAAABU/y4Yv-P85NuU/s64/photo.jpg","userId":"08099351609548175475"}},"outputId":"26546458-6bc4-478f-9e70-a5073989c368"},"source":["device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","model.to(device)\n","summary(model, input_size=(1,5))"],"execution_count":6,"outputs":[{"output_type":"stream","text":["=================================================================\n","Layer (type:depth-idx)                   Param #\n","=================================================================\n","├─Linear: 1-1                            55\n","├─Linear: 1-2                            176\n","├─Linear: 1-3                            208\n","├─Linear: 1-4                            104\n","├─Linear: 1-5                            32\n","=================================================================\n","Total params: 575\n","Trainable params: 575\n","Non-trainable params: 0\n","=================================================================\n"],"name":"stdout"},{"output_type":"execute_result","data":{"text/plain":["=================================================================\n","Layer (type:depth-idx)                   Param #\n","=================================================================\n","├─Linear: 1-1                            55\n","├─Linear: 1-2                            176\n","├─Linear: 1-3                            208\n","├─Linear: 1-4                            104\n","├─Linear: 1-5                            32\n","=================================================================\n","Total params: 575\n","Trainable params: 575\n","Non-trainable params: 0\n","================================================================="]},"metadata":{"tags":[]},"execution_count":6}]},{"cell_type":"markdown","metadata":{"id":"UEHGmlOR4rZH"},"source":["### 3. Provide calculation for the exact number of parameters of the MLP (also in the case of bias terms)"]},{"cell_type":"markdown","metadata":{"id":"cOGim52d4rk8"},"source":["The total number of parameters of the MLP is given by the total number of entries of the matrices $W_1, W_2, W_3, W_4, W_5$, that is \n","\n","\n","$ N^{\\text{w/o  bias}}_{\\text{parameters}} = (11 \\cdot 5) + (16 \\cdot 11 ) + (13 \\cdot 16) + (8 \\cdot 13) + (4 \\cdot 8) = 575$\n","\n","\n","If we would have considered bias terms into our MLP model, we would require additional vectors of parameters $b_1, b_2, b_3, b_4, b_5$ of dimension equal to the output dimension of each layer. In that case the number of parameters will then be\n","\n","\n","$ N^{\\text{w/ bias}}_{\\text{parameters}} = (11 \\cdot 5 + 11) + (16 \\cdot 11 + 16) + (13 \\cdot 16 + 13) + (8 \\cdot 13 + 8) + (4 \\cdot 8 + 4) = 627$\n","\n"]},{"cell_type":"markdown","metadata":{"id":"KogpsgrV4ryd"},"source":["### 4. Calculate the L1 and L2 norm of parameters for the params of each layer\n","\n"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"1kG9faT5ZdlH","executionInfo":{"status":"ok","timestamp":1616246585989,"user_tz":-60,"elapsed":1021,"user":{"displayName":"Roberto Corti","photoUrl":"https://lh4.googleusercontent.com/-KaiJHEP6Eps/AAAAAAAAAAI/AAAAAAAAABU/y4Yv-P85NuU/s64/photo.jpg","userId":"08099351609548175475"}},"outputId":"f773c4b3-1ecf-4e94-d4e4-faf33f1d89a2"},"source":["for index, module in enumerate(model.children()):\n","\n","  with torch.no_grad():\n","    w = module.weight\n","    norm_w = torch.linalg.norm(w, ord=2).item()\n","    print(f'Layer {index+1}\\nL2norm(w) = {norm_w:.3f}')\n","    norm_w = torch.linalg.norm(w, ord=1).item()\n","    print(f'L1norm(w) = {norm_w:.3f}\\n')"],"execution_count":10,"outputs":[{"output_type":"stream","text":["Layer 1\n","L2norm(w) = 1.235\n","L1norm(w) = 3.603\n","\n","Layer 2\n","L2norm(w) = 1.102\n","L1norm(w) = 3.023\n","\n","Layer 3\n","L2norm(w) = 0.904\n","L1norm(w) = 1.829\n","\n","Layer 4\n","L2norm(w) = 0.880\n","L1norm(w) = 1.317\n","\n","Layer 5\n","L2norm(w) = 0.748\n","L1norm(w) = 1.117\n","\n"],"name":"stdout"}]}]}